
@article{xu_importance_nodate,
	title = {Importance of {Data} and {Controllability} in {Neural} {Text} {Simplification}},
	abstract = {Natural language generation has become one of the fastest-growing areas in NLP and a popular playground for studying deep learning techniques. Many variants of sequence-to-sequence models with complicated components have been developed. Yet, as I will demonstrate in this talk, creating high-quality training data and injecting linguistic knowledge can lead to significant performance improvements that overshadow gains from many of these model variants. I will present two recent works from my group on text simplification, a task that requires both lexical and syntactic paraphrasing to improve text accessibility: 1) a neural conditional random field (CRF) based semantic model [1, 2] to create parallel training data [3]; 2) a controllable text generation approach [4] that incorporates syntax through pairwise ranking and data argumentation.},
	language = {en},
	author = {Xu, Wei},
	pages = {2},
	file = {Xu - Importance of Data and Controllability in Neural T.pdf:C\:\\Users\\Cleme\\Zotero\\storage\\74R83H66\\Xu - Importance of Data and Controllability in Neural T.pdf:application/pdf},
}
